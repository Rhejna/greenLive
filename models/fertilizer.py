# -*- coding: utf-8 -*-
"""Fertilizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BXfmG7pB2_x3FYf8Yc_rSZ8wlGwGJ3jU

Ceci est une notebook pour entrainer un modèle qui va prédire le type d'engrais à utiliser et la quantité d'engrais à verser à un instant T.

### Télécharger le jeu de données
"""

!gdown 1BRyffPh9EccacQx_R0Ox9Mel9UZu9-rz

"""## Analyse exploratoire du jeu de données"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv("fertilizer.csv")
data.head()

# Dimensions du jeu de données
print("Le jeu de données a", data.shape[0], "lignes et", data.shape[1], "colonnes")

# Verifions si il y'a des valeurs nulles quelque part
data.isna().any()

"""Nous observons qu'il y'a un caractère ";" partout sur la dernière colonne. Nous allons la retirer et convertir le type des éléments en numérique.
### Prétraitement initial
"""

# Fonction pour retirer un caractère d'une chaine de caractères
clear = lambda string, symbol: string.replace(symbol, "")

# Retirer le caractère ";" du nom de la dernière colonne
data.columns = data.columns[:-1].to_list() + [clear(data.columns[-1], ";")]
# Retirer le caractère ";" sur tous les éléments de la dernière colonne
data['Quantity Kg/hapH'] = data['Quantity Kg/hapH'].apply(lambda x: clear(x, ";"))
# Changer lt type de tous les éléments de la dernière colonne en numérique
data['Quantity Kg/hapH'] = data['Quantity Kg/hapH'].apply(lambda x: float(x))

data.head() # Afficher un bout de nos données

# Observer le type de chaque colonne
data.dtypes

# Retirer les espaces vides dans les colonnes objets
data[["Soil Type", "Fertilizer Name", "Crop Type"]] = data[["Soil Type", "Fertilizer Name", "Crop Type"]].apply(lambda x: x.str.strip())
# Mettre tous les caractères en minuscule dans ces colonnes
data[["Soil Type", "Fertilizer Name", "Crop Type"]] = data[["Soil Type", "Fertilizer Name", "Crop Type"]].apply(lambda x: x.str.lower())

"""Maintenant nous allons analyser le contenu de nos différentes colonnes, en répondant à quelques questions.

### Colonne *Soil Type*
"""

# Combien de types de sol y'a-t-il ?
print("Il y'a", data.iloc[:, 4].unique().size, "types de sol (Sans s'assurer du nettoyage)")
# Quels sont ces types de sol ?
print("Les types de sol sont :", *sorted(data.iloc[:, 4].unique()), sep=" - ")

"""Nous pouvons observer que "loam" et "loamy" apparaissent séparément, pourtant ils constituent la même matière nous allons corriger ce problème en mettant simplement loam"""

# Remplacer "loamy" par "loam"
data["Soil Type"] = data["Soil Type"].apply(lambda x: (x if x != "loamy" else "loam"))

print("Il y'a", data.iloc[:, 4].unique().size, "types de sol")
print("Les types de sol sont :", *sorted(data.iloc[:, 4].unique()), sep=" - ")

"""### Colonne *Crop Type*"""

# Combien de type de cultures y'a t'il ?
print("Il y'a", data["Crop Type"].unique().size, "types de sol")
# Quels sont ces types de cultures ?
print("Les types de cultures sont :", *data["Crop Type"].unique(), sep=" - ")

"""Il n'a pas de redundance sémantique donc c'est OK

### Colonne *Fertilizer Name*
"""

# Combien de type de fertilizer y'a t'il ?
print("Il y'a", data["Fertilizer Name"].unique().size, "types d'engrais")
# Quels sont ces types d'engrais
print("Les types d'engrais sont :", *sorted(data["Fertilizer Name"].unique()), sep=" :: ")

"""(Domain specific) We will convert fertilizer types like "urea" to their NPK ratio "46-0-0" their percentage in the consitituent molecule to deduce the mass of Phosphorus and Potassium. \\
For example, an $18−51−20$ fertilizer contains by weight:
* 18% elemental nitrogen,
* 0.436 × 51 = 22% elemental phosphorus, and
* 0.83 × 20 = 17% elemental potassium.
"""

# Commencons par retirer le prefixe "npk" devant certain ratio
data["Fertilizer Name"] = data["Fertilizer Name"].apply(lambda x: x.replace("npk", "").strip())

## Verifions si les prefixes ont été retirés
# Combien de type de fertilizer y'a t'il ?
print("Il y'a", data["Fertilizer Name"].unique().size, "types d'engrais")
# Quels sont ces types d'engrais
print("Les types d'engrais sont :", *sorted(data["Fertilizer Name"].unique()), sep=" :: ")

npk_ratios = {
    "urea": "46", # 46-0-0
    "ammonium nitrate": "33",
    "bone meal": "4-12", # 4-12-0
    "compound fertilizer": "unknown",
    "dap": "18-46",    # https://pediaa.com/what-is-the-difference-between-dap-and-npk-fertilizer/
    "nitrophos": "19-4-10",   # https://www.solutionsstores.com/nitrophos-superturf-fertilizer-19-4-10
    "organic_fertilizer": "unknown",
    "phosphate rock": "0-3-0",
    "ssp": "0-16-0",  # https://guide2agriculture.com/single-super-phosphate/
    "sulphate of potash": "0-0-50"
}

# Expression reguliere pour reconnaitre le format du ratio NPK
import re
npk_pattern = re.compile(r'^\d+(-\d+)*(-\d+)*$')
# Utilisation : npk_pattern.match(str)

# Fonction pour transformer les elements de la colonne fs
def trans_fs(x):
  if npk_pattern.match(x):
    return x
  elif x in npk_ratios.keys():
    return npk_ratios[x]
  else:
    return "unknown"

trans_fs("urea")

# Transformer tous les éléments de la colonne "Fertilizer Name"
data["Fertilizer Name"] = data["Fertilizer Name"].apply(trans_fs)

## Verifions si les prefixes ont été retirés
# Combien de type de fertilizer y'a t'il ?
print("Il y'a", data["Fertilizer Name"].unique().size, "types d'engrais")
# Quels sont ces types d'engrais
print("Les types d'engrais sont :", *sorted(data["Fertilizer Name"].unique()), sep=" :: ")

"""C'est OK. Maintenant nous allons séparer les ratios pour créer une colonne nF, pF, kF qui sont les quantités élémentaires de nitrogène, phosphore et de potassium."""

# Constituant de fertilisant
nF, pF, kF = const_fertilizer = [list() for i in range(3)]
element_prop = [1, 0.436, 0.83]

# Place the ratios in the columns nf, pF, kF
for i in range(data.shape[0]):
  # Get the ratio
  ratio = data["Fertilizer Name"].iloc[i]
  if ratio=="unknown":
    # If the ratio is unknown simply add -1 everywhere
    ratio = ["-1" for i in range(3)]
    # Add the ratio
    for i in range(len(ratio)):
      const_fertilizer[i].append(int(ratio[i])*element_prop[i])
  else:
    ratio = ratio.split("-")
    # Add the ratio
    for i in range(len(ratio)):
      const_fertilizer[i].append(int(ratio[i])*element_prop[i])
    else: # Some ratios are absent - Meaning zero
      i+=1
      while i<3:
        const_fertilizer[i].append(0)
        i+=1

len(nF), len(pF), len(kF)

data["nF"] = nF
data["pF"] = pF
data["kF"] = kF

# Replace all the negative values with the mode of the column
data[["nF", "pF", "kF"]] = data[["nF", "pF", "kF"]].apply(lambda x: x.apply(lambda xi: xi if xi!=-1 else x[x!=-1].median()))

data.head()

"""C'est bon pour la colonne *Fertilizer Name*. Nous pouvons passer à l'entrainement du modèle

## Entrainement du modèle

Nous créons un dataset d'entrainement où nous allons retirer la colonne "Fertilizer Name" vu qu'elle n'est plus utile
"""

model_data = data.drop(["Fertilizer Name"], axis=1)
model_data.head()

# Nous séparons les caractéristiques et la cible
features = model_data.drop(["Quantity Kg/hapH"], axis=1)
target = model_data[["Quantity Kg/hapH"]]

"""### Séparation du jeu de données en train-test"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2)

"""### Importation des libraries importantes"""

# Les encodeurs
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer # make_column_transformer

# Les algorithmes de machine learning
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from sklearn.svm import SVR

# La fonction pour créer les pipelines
from sklearn.pipeline import make_pipeline

"""### Création du préprocesseur"""

# Separer les données catégorique et les données numériques
categorical_features = ["Soil Type", "Crop Type"]
numerical_features = list(set(x_train.columns)-set(categorical_features))

numerical_features

# Créer le préprocesseur
preprocessor = ColumnTransformer([
    ('one_hot_encoder', OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ('standard_scaler', StandardScaler(), numerical_features)
])

"""### Validation du modèle

#### Définition des modèles

##### Decision Tree Regressor
"""

dtr = make_pipeline(
    *[
        preprocessor,
        DecisionTreeRegressor()
    ]
)

"""##### Linear Regression"""

lnr = make_pipeline(
    *[
        preprocessor,
        LinearRegression()
    ]
)

mlp = make_pipeline(
    *[
        preprocessor,
        MLPRegressor(max_iter=10000)
    ]
)

rfr = make_pipeline(
    *[
        preprocessor,
        RandomForestRegressor()
    ]
)

abr = make_pipeline(
    *[
        preprocessor,
        AdaBoostRegressor()
    ]
)

svr = make_pipeline(
    *[
        preprocessor,
        SVR()
    ]
)

"""#### Validation des modèles

##### validation du dtr
"""

# Entrainement du modèle dtr
dtr.fit(x_train, y_train);

# Entrainement du modèle lnr
lnr.fit(x_train, y_train);

# Entrainement du modèle mlp
mlp.fit(x_train, y_train.values.reshape(-1,));

# Entrainement du rfr
rfr.fit(x_train, y_train.values.reshape(-1,));

abr.fit(x_train, y_train.values)

svr.fit(x_train, y_train.values)

"""###### Scores
By defaut, regressors use the r2_score function as metric. It compares the model with the basic horizontal line
"""

dtr.score(x_test, y_test)

lnr.score(x_test, y_test)

mlp.score(x_test, y_test)

rfr.score(x_test, y_test)

abr.score(x_test, y_test)

svr.score(x_test, y_test)

"""## Save the best model"""

import pickle
with open("fertilizer.pkl", "wb") as file:
  pickle.dump(rfr, file)

"""## Using the saved model"""

import pickle
import sklearn

# Load the model
with open("fertilizer.pkl", "rb") as file:
  model = pickle.load(file)

x_train.to_csv("x_train.csv")

model.predict(pd.DataFrame.from_dict(x_train.to_dict()))

import json

instance = x_train.iloc[0, :].to_dict()
string = json.dumps(instance)
string

"""# Gerome"""

# Helpers
# Get the data of the user
def get_user_data():
  index = np.random.randint(0, data.shape[0])
  return data.iloc[index, :].to_dict().__str__()

get_user_data()

!pip install openai

import openai

# Configure the API_KEY
openai.api_key = api_key = API_KEY = "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"

## Create the context for the model
# Personality
identity = "Gérome"
creators = "AI developpers and experienced farmers from GREENLIVE"
mission = f"an experienced AI farmer developped by {creators} and your role is to help farmers to understand the data in their farm"
# Context
context = {
    "role": "system",
    "content": f"Your name is {identity}. You where created by {creators}. You are {mission}. Answer based on latest data record from here : {get_user_data()}"
}

# Provide the context initially
messages = [context]
# Start the chat
while True:
  # Prompt the user
  question = input("User : "); print()
  if question:
    # Insert the question to messages
    messages.append({
        "role": "user",
        "content": question
    })
    # Prompt chatGPT
    chat = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages
    )
    # Extract the reply
    reply = chat.choices[0].message.content
    # print the message
    reply = reply.replace('. ', '.\n')
    print(f"Gerome : {reply}\n")
  else:
    break